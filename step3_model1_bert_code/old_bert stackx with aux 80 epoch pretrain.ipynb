{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# !mv ~/stackx_base_with_aux_ep_051_val_perplexity_4.74_val_spearman_0.33.pth uploaded/stackx-bert-base/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "I0224 13:15:21.951596 4573377984 file_utils.py:37] PyTorch version 1.3.0.post2 available.\n",
      "I0224 13:15:25.412374 4573377984 file_utils.py:51] TensorFlow version 2.0.0 available.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-becbf42a218b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from pytorch_transformers import BertForSequenceClassification, PretrainedConfig\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import pprint\n",
    "import numpy as np\n",
    "import itertools\n",
    "import functools\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from poutyne.framework import EarlyStopping\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Text\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from data import (QuestDataset,\n",
    "                  ALL_TARGETS,\n",
    "                  QUESTION_TARGETS,\n",
    "                  EASY_ANSWER_TARGETS,\n",
    "                  EASY_QUESTION_TARGETS,\n",
    "                  HARD_ANSWER_TARGETS, \n",
    "                  HARD_QUESTION_TARGETS)\n",
    "from data.augmentation import BertRandomTokenizer\n",
    "from metrics import Spearman, spearman_metric\n",
    "from callbacks import LosswiseCallback, CSVParamLogger\n",
    "from utils import transform_target_columns_to_ordinals\n",
    "from models import BertForQuestRegression\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "tokenizer = BertTokenizer('uploaded/stackx-bert-base/stackx-base-cased-vocab.txt', do_lower_case=False)\n",
    "\n",
    "def get_model():\n",
    "    config = BertConfig.from_json_file('uploaded/stackx-bert-base/stackx-base-cased-config.json')\n",
    "    config.__dict__['num_labels'] = len(ALL_TARGETS)\n",
    "    \n",
    "    model = BertForQuestRegression(config)\n",
    "    state_dict = torch.load('uploaded/stackx-bert-base/stackx_base_with_aux_ep_080_val_perplexity_4.39_val_spearman_0.34.pth',\n",
    "                   map_location='cpu'\n",
    "        ) \n",
    "    state_dict = {name.replace('LayerNorm.gamma', 'LayerNorm.weight').replace('LayerNorm.beta', 'LayerNorm.bias'): weight\n",
    "                  for name, weight in state_dict.items()}\n",
    "    \n",
    "    del state_dict['classifier.weight']\n",
    "    del state_dict['classifier.bias']\n",
    "    model.load_state_dict(state_dict, strict = False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from metrics import Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "folds = pd.read_csv('data/folds.csv')['fold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from poutyne.framework import Model, ModelCheckpoint, CSVLogger\n",
    "from transformers import AdamW\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "loader_kws = dict(batch_size=4, num_workers=1)\n",
    "\n",
    "\n",
    "checkpoint_dir = Path('/storage/monty/quest_challenge/stackx_with_aux_80/')\n",
    "\n",
    "if not checkpoint_dir.exists():\n",
    "    os.makedirs(str(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 415.85s Step 1221/1221: loss: 0.384714, spearman: 0.297420, val_loss: 0.376921, val_spearman: 0.374295\n",
      "Epoch 2/8 405.40s Step 1221/1221: loss: 0.358942, spearman: 0.396235, val_loss: 0.371795, val_spearman: 0.397263\n",
      "Epoch 3/8 410.63s Step 1221/1221: loss: 0.345954, spearman: 0.446971, val_loss: 0.370101, val_spearman: 0.404358\n",
      "Epoch 4/8 410.17s Step 1221/1221: loss: 0.332468, spearman: 0.494726, val_loss: 0.375871, val_spearman: 0.397839\n",
      "Epoch 5/8 410.22s Step 1221/1221: loss: 0.318069, spearman: 0.542985, val_loss: 0.379286, val_spearman: 0.394113\n",
      "Epoch 6/8 406.52s Step 1221/1221: loss: 0.303217, spearman: 0.585413, val_loss: 0.391867, val_spearman: 0.383968\n",
      "Epoch 7/8 407.00s Step 1221/1221: loss: 0.291089, spearman: 0.617667, val_loss: 0.402429, val_spearman: 0.376874\n",
      "Epoch 8/8 416.18s Step 1221/1221: loss: 0.282086, spearman: 0.642389, val_loss: 0.400215, val_spearman: 0.380890\n",
      "Epoch 1/8 421.11s Step 1220/1220: loss: 0.387219, spearman: 0.289662, val_loss: 0.371856, val_spearman: 0.380941\n",
      "Epoch 2/8 ETA 246s Step 460/1220: loss: 0.350196"
     ]
    }
   ],
   "source": [
    "spearman = Spearman(ALL_TARGETS)\n",
    "for fold_idx in range(5):\n",
    "    train_frame, val_frame = train_df[folds != fold_idx], train_df[folds == fold_idx]\n",
    "\n",
    "    train_dataset = QuestDataset(train_frame, tokenizer, max_seg_length=512)\n",
    "    val_dataset = QuestDataset(val_frame, tokenizer, max_seg_length=512)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, **loader_kws)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, **loader_kws)\n",
    "    \n",
    "    model = get_model()\n",
    "    trainer = Model(model, Adam(model.parameters(), lr = 3e-5), 'bce_with_logits', epoch_metrics=[spearman])\n",
    "    trainer.to('cuda')\n",
    "    \n",
    "    \n",
    "    checkpoint_name = 'stackx_with_aux_80_fold_'+str(fold_idx)+'_ep_{epoch:03}_val_spearman_{val_spearman:.2f}.pth'\n",
    "    callbacks =[\n",
    "        spearman.callback,\n",
    "        ModelCheckpoint(str(checkpoint_dir / checkpoint_name), atomic_write=False),\n",
    "        CSVParamLogger(str(checkpoint_dir / 'training_log.csv'), append= fold_idx!=0 , extra_metrics=ALL_TARGETS)\n",
    "    ]\n",
    "    \n",
    "    history = trainer.fit_generator(\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        epochs=8,\n",
    "        callbacks = callbacks, \n",
    "    )\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackx_with_aux_fold_0_ep_001_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_fold_0_ep_002_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_fold_0_ep_003_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_fold_0_ep_004_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_fold_0_ep_005_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_0_ep_006_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_0_ep_007_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_fold_0_ep_008_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_fold_1_ep_001_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_fold_1_ep_002_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_fold_1_ep_003_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_fold_1_ep_004_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_fold_1_ep_005_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_1_ep_006_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_1_ep_007_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_fold_1_ep_008_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_fold_2_ep_001_val_spearman_0.36.pth\r\n",
      "stackx_with_aux_fold_2_ep_002_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_2_ep_003_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_2_ep_004_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_2_ep_005_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_fold_2_ep_006_val_spearman_0.36.pth\r\n",
      "stackx_with_aux_fold_2_ep_007_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_fold_2_ep_008_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_fold_3_ep_001_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_fold_3_ep_002_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_3_ep_003_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_3_ep_004_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_3_ep_005_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_3_ep_006_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_fold_3_ep_007_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_fold_3_ep_008_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_fold_4_ep_001_val_spearman_0.36.pth\r\n",
      "stackx_with_aux_fold_4_ep_002_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_4_ep_003_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_fold_4_ep_004_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_fold_4_ep_005_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_fold_4_ep_006_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_fold_4_ep_007_val_spearman_0.36.pth\r\n",
      "stackx_with_aux_fold_4_ep_008_val_spearman_0.36.pth\r\n",
      "training_log.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /storage/monty/quest_challenge/stackx_with_aux/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "!mkdir /storage/monty/quest_challenge/submits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "!mkdir /storage/monty/quest_challenge/submits/stackx_aux_ep_005_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "!cp /storage/monty/quest_challenge/stackx_with_aux/*ep_005* /storage/monty/quest_challenge/submits/stackx_aux_ep_005_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "!cp uploaded/stackx-bert-base/stackx-base-cased-config.json /storage/monty/quest_challenge/submits/stackx_aux_ep_005_submit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "summary = pd.read_csv('/storage/monty/quest_challenge/stackx_with_aux_80/training_log.csv').groupby('epoch').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lr</th>\n",
       "      <th>loss</th>\n",
       "      <th>spearman</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_spearman</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>413.262449</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.386173</td>\n",
       "      <td>0.297299</td>\n",
       "      <td>0.371963</td>\n",
       "      <td>0.376791</td>\n",
       "      <td>0.345527</td>\n",
       "      <td>0.614576</td>\n",
       "      <td>0.392163</td>\n",
       "      <td>0.272770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484610</td>\n",
       "      <td>0.169387</td>\n",
       "      <td>0.379546</td>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.111699</td>\n",
       "      <td>0.263339</td>\n",
       "      <td>0.749182</td>\n",
       "      <td>0.269192</td>\n",
       "      <td>0.676294</td>\n",
       "      <td>0.120631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>410.816827</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.358977</td>\n",
       "      <td>0.402029</td>\n",
       "      <td>0.368151</td>\n",
       "      <td>0.398070</td>\n",
       "      <td>0.374610</td>\n",
       "      <td>0.624733</td>\n",
       "      <td>0.404160</td>\n",
       "      <td>0.297390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510031</td>\n",
       "      <td>0.231108</td>\n",
       "      <td>0.399606</td>\n",
       "      <td>0.138805</td>\n",
       "      <td>0.161617</td>\n",
       "      <td>0.328969</td>\n",
       "      <td>0.760578</td>\n",
       "      <td>0.286568</td>\n",
       "      <td>0.684135</td>\n",
       "      <td>0.163946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>411.284624</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.345241</td>\n",
       "      <td>0.453302</td>\n",
       "      <td>0.368524</td>\n",
       "      <td>0.400611</td>\n",
       "      <td>0.379657</td>\n",
       "      <td>0.636339</td>\n",
       "      <td>0.404208</td>\n",
       "      <td>0.273260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506188</td>\n",
       "      <td>0.255536</td>\n",
       "      <td>0.408861</td>\n",
       "      <td>0.155035</td>\n",
       "      <td>0.183971</td>\n",
       "      <td>0.337509</td>\n",
       "      <td>0.758290</td>\n",
       "      <td>0.284228</td>\n",
       "      <td>0.681036</td>\n",
       "      <td>0.160476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>410.717383</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.331346</td>\n",
       "      <td>0.502631</td>\n",
       "      <td>0.373107</td>\n",
       "      <td>0.393716</td>\n",
       "      <td>0.361959</td>\n",
       "      <td>0.628755</td>\n",
       "      <td>0.401346</td>\n",
       "      <td>0.266957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499248</td>\n",
       "      <td>0.245255</td>\n",
       "      <td>0.397475</td>\n",
       "      <td>0.158187</td>\n",
       "      <td>0.174373</td>\n",
       "      <td>0.333453</td>\n",
       "      <td>0.752996</td>\n",
       "      <td>0.271340</td>\n",
       "      <td>0.677399</td>\n",
       "      <td>0.152197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>409.851900</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.316308</td>\n",
       "      <td>0.549825</td>\n",
       "      <td>0.379911</td>\n",
       "      <td>0.387125</td>\n",
       "      <td>0.353607</td>\n",
       "      <td>0.629211</td>\n",
       "      <td>0.399839</td>\n",
       "      <td>0.240614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494678</td>\n",
       "      <td>0.249538</td>\n",
       "      <td>0.403154</td>\n",
       "      <td>0.157577</td>\n",
       "      <td>0.183019</td>\n",
       "      <td>0.332985</td>\n",
       "      <td>0.743429</td>\n",
       "      <td>0.259349</td>\n",
       "      <td>0.664555</td>\n",
       "      <td>0.156798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>409.102902</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.301744</td>\n",
       "      <td>0.591566</td>\n",
       "      <td>0.389039</td>\n",
       "      <td>0.379273</td>\n",
       "      <td>0.352935</td>\n",
       "      <td>0.626970</td>\n",
       "      <td>0.393321</td>\n",
       "      <td>0.228109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481139</td>\n",
       "      <td>0.230063</td>\n",
       "      <td>0.390132</td>\n",
       "      <td>0.140585</td>\n",
       "      <td>0.160397</td>\n",
       "      <td>0.315007</td>\n",
       "      <td>0.740578</td>\n",
       "      <td>0.229250</td>\n",
       "      <td>0.670173</td>\n",
       "      <td>0.151370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>409.644925</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.290059</td>\n",
       "      <td>0.623470</td>\n",
       "      <td>0.397447</td>\n",
       "      <td>0.376278</td>\n",
       "      <td>0.343656</td>\n",
       "      <td>0.625906</td>\n",
       "      <td>0.388901</td>\n",
       "      <td>0.224848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481518</td>\n",
       "      <td>0.235266</td>\n",
       "      <td>0.384552</td>\n",
       "      <td>0.139922</td>\n",
       "      <td>0.170470</td>\n",
       "      <td>0.330018</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>0.216687</td>\n",
       "      <td>0.666764</td>\n",
       "      <td>0.154216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>412.065767</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.281307</td>\n",
       "      <td>0.646360</td>\n",
       "      <td>0.402349</td>\n",
       "      <td>0.373482</td>\n",
       "      <td>0.326928</td>\n",
       "      <td>0.627559</td>\n",
       "      <td>0.383275</td>\n",
       "      <td>0.228420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476666</td>\n",
       "      <td>0.227535</td>\n",
       "      <td>0.385765</td>\n",
       "      <td>0.133732</td>\n",
       "      <td>0.167929</td>\n",
       "      <td>0.321699</td>\n",
       "      <td>0.738979</td>\n",
       "      <td>0.221496</td>\n",
       "      <td>0.672977</td>\n",
       "      <td>0.156306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time       lr      loss  spearman  val_loss  val_spearman  \\\n",
       "epoch                                                                    \n",
       "1      413.262449  0.00003  0.386173  0.297299  0.371963      0.376791   \n",
       "2      410.816827  0.00003  0.358977  0.402029  0.368151      0.398070   \n",
       "3      411.284624  0.00003  0.345241  0.453302  0.368524      0.400611   \n",
       "4      410.717383  0.00003  0.331346  0.502631  0.373107      0.393716   \n",
       "5      409.851900  0.00003  0.316308  0.549825  0.379911      0.387125   \n",
       "6      409.102902  0.00003  0.301744  0.591566  0.389039      0.379273   \n",
       "7      409.644925  0.00003  0.290059  0.623470  0.397447      0.376278   \n",
       "8      412.065767  0.00003  0.281307  0.646360  0.402349      0.373482   \n",
       "\n",
       "       question_asker_intent_understanding  question_body_critical  \\\n",
       "epoch                                                                \n",
       "1                                 0.345527                0.614576   \n",
       "2                                 0.374610                0.624733   \n",
       "3                                 0.379657                0.636339   \n",
       "4                                 0.361959                0.628755   \n",
       "5                                 0.353607                0.629211   \n",
       "6                                 0.352935                0.626970   \n",
       "7                                 0.343656                0.625906   \n",
       "8                                 0.326928                0.627559   \n",
       "\n",
       "       question_conversational  question_expect_short_answer  ...  \\\n",
       "epoch                                                         ...   \n",
       "1                     0.392163                      0.272770  ...   \n",
       "2                     0.404160                      0.297390  ...   \n",
       "3                     0.404208                      0.273260  ...   \n",
       "4                     0.401346                      0.266957  ...   \n",
       "5                     0.399839                      0.240614  ...   \n",
       "6                     0.393321                      0.228109  ...   \n",
       "7                     0.388901                      0.224848  ...   \n",
       "8                     0.383275                      0.228420  ...   \n",
       "\n",
       "       question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "epoch                                                                       \n",
       "1                   0.484610        0.169387                     0.379546   \n",
       "2                   0.510031        0.231108                     0.399606   \n",
       "3                   0.506188        0.255536                     0.408861   \n",
       "4                   0.499248        0.245255                     0.397475   \n",
       "5                   0.494678        0.249538                     0.403154   \n",
       "6                   0.481139        0.230063                     0.390132   \n",
       "7                   0.481518        0.235266                     0.384552   \n",
       "8                   0.476666        0.227535                     0.385765   \n",
       "\n",
       "       answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "epoch                                                            \n",
       "1              0.071851          0.111699             0.263339   \n",
       "2              0.138805          0.161617             0.328969   \n",
       "3              0.155035          0.183971             0.337509   \n",
       "4              0.158187          0.174373             0.333453   \n",
       "5              0.157577          0.183019             0.332985   \n",
       "6              0.140585          0.160397             0.315007   \n",
       "7              0.139922          0.170470             0.330018   \n",
       "8              0.133732          0.167929             0.321699   \n",
       "\n",
       "       answer_type_instructions  answer_type_procedure  \\\n",
       "epoch                                                    \n",
       "1                      0.749182               0.269192   \n",
       "2                      0.760578               0.286568   \n",
       "3                      0.758290               0.284228   \n",
       "4                      0.752996               0.271340   \n",
       "5                      0.743429               0.259349   \n",
       "6                      0.740578               0.229250   \n",
       "7                      0.739412               0.216687   \n",
       "8                      0.738979               0.221496   \n",
       "\n",
       "       answer_type_reason_explanation  answer_well_written  \n",
       "epoch                                                       \n",
       "1                            0.676294             0.120631  \n",
       "2                            0.684135             0.163946  \n",
       "3                            0.681036             0.160476  \n",
       "4                            0.677399             0.152197  \n",
       "5                            0.664555             0.156798  \n",
       "6                            0.670173             0.151370  \n",
       "7                            0.666764             0.154216  \n",
       "8                            0.672977             0.156306  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                                     0\n",
       "lr                                       0\n",
       "loss                                     0\n",
       "spearman                                 7\n",
       "val_loss                                 7\n",
       "val_spearman                             2\n",
       "question_asker_intent_understanding      2\n",
       "question_body_critical                   2\n",
       "question_conversational                  2\n",
       "question_expect_short_answer             1\n",
       "question_fact_seeking                    2\n",
       "question_has_commonly_accepted_answer    1\n",
       "question_interestingness_others          2\n",
       "question_interestingness_self            2\n",
       "question_multi_intent                    2\n",
       "question_not_really_a_question           3\n",
       "question_opinion_seeking                 1\n",
       "question_type_choice                     2\n",
       "question_type_compare                    3\n",
       "question_type_consequence                3\n",
       "question_type_definition                 1\n",
       "question_type_entity                     3\n",
       "question_type_instructions               1\n",
       "question_type_procedure                  1\n",
       "question_type_reason_explanation         1\n",
       "question_type_spelling                   4\n",
       "question_well_written                    1\n",
       "answer_helpful                           2\n",
       "answer_level_of_information              2\n",
       "answer_plausible                         3\n",
       "answer_relevance                         2\n",
       "answer_satisfaction                      2\n",
       "answer_type_instructions                 1\n",
       "answer_type_procedure                    1\n",
       "answer_type_reason_explanation           1\n",
       "answer_well_written                      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(summary.values.argmax(0), index=summary.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {},
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>411.284624</td>\n",
       "      <td>413.262449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.345241</td>\n",
       "      <td>0.386173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman</th>\n",
       "      <td>0.453302</td>\n",
       "      <td>0.646360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_loss</th>\n",
       "      <td>0.368524</td>\n",
       "      <td>0.402349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_spearman</th>\n",
       "      <td>0.400611</td>\n",
       "      <td>0.400611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <td>0.379657</td>\n",
       "      <td>0.379657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_body_critical</th>\n",
       "      <td>0.636339</td>\n",
       "      <td>0.636339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_conversational</th>\n",
       "      <td>0.404208</td>\n",
       "      <td>0.404208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <td>0.273260</td>\n",
       "      <td>0.297390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <td>0.355161</td>\n",
       "      <td>0.355161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <td>0.417949</td>\n",
       "      <td>0.424237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <td>0.359731</td>\n",
       "      <td>0.359731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <td>0.488388</td>\n",
       "      <td>0.488388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_multi_intent</th>\n",
       "      <td>0.569081</td>\n",
       "      <td>0.569081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <td>0.074065</td>\n",
       "      <td>0.082163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <td>0.449984</td>\n",
       "      <td>0.459107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type_choice</th>\n",
       "      <td>0.725908</td>\n",
       "      <td>0.725908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type_compare</th>\n",
       "      <td>0.353975</td>\n",
       "      <td>0.357128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type_consequence</th>\n",
       "      <td>0.167987</td>\n",
       "      <td>0.171841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type_definition</th>\n",
       "      <td>0.353896</td>\n",
       "      <td>0.355608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type_entity</th>\n",
       "      <td>0.445206</td>\n",
       "      <td>0.447734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type_instructions</th>\n",
       "      <td>0.774848</td>\n",
       "      <td>0.778114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type_procedure</th>\n",
       "      <td>0.333209</td>\n",
       "      <td>0.341567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <td>0.659119</td>\n",
       "      <td>0.659664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type_spelling</th>\n",
       "      <td>0.065219</td>\n",
       "      <td>0.067613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_well_written</th>\n",
       "      <td>0.506188</td>\n",
       "      <td>0.510031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_helpful</th>\n",
       "      <td>0.255536</td>\n",
       "      <td>0.255536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <td>0.408861</td>\n",
       "      <td>0.408861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_plausible</th>\n",
       "      <td>0.155035</td>\n",
       "      <td>0.158187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_relevance</th>\n",
       "      <td>0.183971</td>\n",
       "      <td>0.183971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <td>0.337509</td>\n",
       "      <td>0.337509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <td>0.758290</td>\n",
       "      <td>0.760578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <td>0.284228</td>\n",
       "      <td>0.286568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <td>0.681036</td>\n",
       "      <td>0.684135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_well_written</th>\n",
       "      <td>0.160476</td>\n",
       "      <td>0.163946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                3           0\n",
       "time                                   411.284624  413.262449\n",
       "lr                                       0.000030    0.000030\n",
       "loss                                     0.345241    0.386173\n",
       "spearman                                 0.453302    0.646360\n",
       "val_loss                                 0.368524    0.402349\n",
       "val_spearman                             0.400611    0.400611\n",
       "question_asker_intent_understanding      0.379657    0.379657\n",
       "question_body_critical                   0.636339    0.636339\n",
       "question_conversational                  0.404208    0.404208\n",
       "question_expect_short_answer             0.273260    0.297390\n",
       "question_fact_seeking                    0.355161    0.355161\n",
       "question_has_commonly_accepted_answer    0.417949    0.424237\n",
       "question_interestingness_others          0.359731    0.359731\n",
       "question_interestingness_self            0.488388    0.488388\n",
       "question_multi_intent                    0.569081    0.569081\n",
       "question_not_really_a_question           0.074065    0.082163\n",
       "question_opinion_seeking                 0.449984    0.459107\n",
       "question_type_choice                     0.725908    0.725908\n",
       "question_type_compare                    0.353975    0.357128\n",
       "question_type_consequence                0.167987    0.171841\n",
       "question_type_definition                 0.353896    0.355608\n",
       "question_type_entity                     0.445206    0.447734\n",
       "question_type_instructions               0.774848    0.778114\n",
       "question_type_procedure                  0.333209    0.341567\n",
       "question_type_reason_explanation         0.659119    0.659664\n",
       "question_type_spelling                   0.065219    0.067613\n",
       "question_well_written                    0.506188    0.510031\n",
       "answer_helpful                           0.255536    0.255536\n",
       "answer_level_of_information              0.408861    0.408861\n",
       "answer_plausible                         0.155035    0.158187\n",
       "answer_relevance                         0.183971    0.183971\n",
       "answer_satisfaction                      0.337509    0.337509\n",
       "answer_type_instructions                 0.758290    0.760578\n",
       "answer_type_procedure                    0.284228    0.286568\n",
       "answer_type_reason_explanation           0.681036    0.684135\n",
       "answer_well_written                      0.160476    0.163946"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([summary.T[3], summary.max()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4036654079056681"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.max()[ALL_TARGETS].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackx_with_aux_80_fold_0_ep_001_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_80_fold_0_ep_002_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_0_ep_003_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_0_ep_004_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_0_ep_005_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_80_fold_0_ep_006_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_0_ep_007_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_0_ep_008_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_1_ep_001_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_1_ep_002_val_spearman_0.41.pth\r\n",
      "stackx_with_aux_80_fold_1_ep_003_val_spearman_0.41.pth\r\n",
      "stackx_with_aux_80_fold_1_ep_004_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_1_ep_005_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_80_fold_1_ep_006_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_1_ep_007_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_1_ep_008_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_2_ep_001_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_80_fold_2_ep_002_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_2_ep_003_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_2_ep_004_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_80_fold_2_ep_005_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_80_fold_2_ep_006_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_2_ep_007_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_2_ep_008_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_3_ep_001_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_3_ep_002_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_3_ep_003_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_80_fold_3_ep_004_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_80_fold_3_ep_005_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_3_ep_006_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_3_ep_007_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_80_fold_3_ep_008_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_80_fold_4_ep_001_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_80_fold_4_ep_002_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_80_fold_4_ep_003_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_4_ep_004_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_4_ep_005_val_spearman_0.38.pth\r\n",
      "stackx_with_aux_80_fold_4_ep_006_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_80_fold_4_ep_007_val_spearman_0.37.pth\r\n",
      "stackx_with_aux_80_fold_4_ep_008_val_spearman_0.36.pth\r\n",
      "training_log.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /storage/monty/quest_challenge/stackx_with_aux_80/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "!cp uploaded/stackx-bert-base/stackx-base-cased-config.json /storage/monty/quest_challenge/submits/stackx_80_aux_ep_003_submit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackx-base-cased-config.json\r\n",
      "stackx-base-cased-vocab.txt\r\n",
      "stackx_with_aux_80_fold_0_ep_004_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_1_ep_004_val_spearman_0.40.pth\r\n",
      "stackx_with_aux_80_fold_2_ep_004_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_80_fold_3_ep_004_val_spearman_0.39.pth\r\n",
      "stackx_with_aux_80_fold_4_ep_004_val_spearman_0.38.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls /storage/monty/quest_challenge/submits/stackx_80_aux_ep_004_submit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
